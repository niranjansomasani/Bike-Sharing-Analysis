{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries(This may take sometime and may need explicit installs through pip/conda or manual installations\n",
    "# if some of the libraries are used for the first time like xgboost etc.,)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from datetime import datetime\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "%matplotlib inline\n",
    "import xgboost\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Loading Data from local\n",
    "train = pd.read_csv('F:/Warpath to Data Science/regression/bike-sharing/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the date column is not imported as date format we can use below command to overcome the mismatch\n",
    "#df = train\n",
    "#df['datetime'] = pd.to_datetime(df['datetime'], format='%d%b%Y')\n",
    "print(\"training data: \", train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 12 columns):\n",
      "datetime      10886 non-null object\n",
      "season        10886 non-null int64\n",
      "holiday       10886 non-null int64\n",
      "workingday    10886 non-null int64\n",
      "weather       10886 non-null int64\n",
      "temp          10886 non-null float64\n",
      "atemp         10886 non-null float64\n",
      "humidity      10886 non-null int64\n",
      "windspeed     10886 non-null float64\n",
      "casual        10886 non-null int64\n",
      "registered    10886 non-null int64\n",
      "count         10886 non-null int64\n",
      "dtypes: float64(3), int64(8), object(1)\n",
      "memory usage: 1020.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that there are no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature Engineering\n",
    "Extrating the hour, day, month, year column from the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new columns for hour, day, month, year from Datetime column\n",
    "temp = pd.DatetimeIndex(train['datetime'])\n",
    "train['year'] = temp.year\n",
    "train['month'] = temp.month\n",
    "train['hour'] = temp.hour\n",
    "train['weekday'] = temp.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#changing year to 0,1 for values 2011, 2012 respectively to be in range with other columns\n",
    "train['year'] = train['year'].map({2011:0, 2012:1})\n",
    "test['year'] = test['year'].map({2011:0, 2012:1})\n",
    "test1['year'] = test1['year'].map({2011:0, 2012:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making sure the extra columns we just created are reflecting\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#changing data types as appropriate\n",
    "categoryVariableList = [\"hour\",\"weekday\",\"month\",\"season\",\"weather\",\"holiday\",\"workingday\", 'year']\n",
    "for var in categoryVariableList:\n",
    "    train[var] = train[var].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logarithmic transformation of the target variable as we have natural outliers\n",
    "train['log_count'] =np.log(train['count']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating indepdent(input) training variables dataset\n",
    "dropFeatures = ['datetime', 'casual', 'registered', 'count', 'log_count']\n",
    "x_train = train.drop(dropFeatures,axis=1)\n",
    "y_train = train[['log_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting into training and test data for registered users\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size = .30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining a function for rmsle which is a criterion used on kaggle to evaluate the model performance\n",
    "def  rmsle(y, y_, convertExp):\n",
    "    if convertExp:\n",
    "        y = np.exp(y)-1,\n",
    "        y_ = np.exp(y_)-1\n",
    "    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n",
    "    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return np.sqrt(np.mean(calc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying Random Forest Model\n",
    "forest = RandomForestRegressor(n_estimators = 1000, random_state=1)\n",
    "#fitting model\n",
    "forest.fit(X_train, Y_train)\n",
    "y_train_pred = forest.predict(X_train)\n",
    "predictions = forest.predict(X_test)\n",
    "\n",
    "#calculating RMSE value\n",
    "Y_test1 = Y_test.values\n",
    "print (\"RMSLE Value For Random Forest: \",rmsle(Y_test1, predictions, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying Gradient Boost Model\n",
    "gbm = GradientBoostingRegressor(n_estimators=4000,alpha=0.01) ### Test 0.41\n",
    "gbm.fit(X_train, Y_train)\n",
    "predictions = gbm.predict(X= X_test)\n",
    "Y_test1 = Y_test.values\n",
    "#preds = preds.values\n",
    "print (\"RMSLE Value For Gradient Boost: \",rmsle(Y_test1, predictions, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying Decision Tree Regressor model\n",
    "clf = DecisionTreeRegressor()\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X= X_test)\n",
    "Y_test1 = Y_test.values\n",
    "#preds = preds.values\n",
    "print (\"RMSLE Value For DecisionTree: \",rmsle(Y_test1, predictions, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying XG Boost Model\n",
    "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "xgb.fit(X_train, Y_train)\n",
    "predictions = xgb.predict(X= X_test)\n",
    "Y_test1 = Y_test.values\n",
    "#preds = preds.values\n",
    "print (\"RMSLE Value For XG Boost Model: \",rmsle(Y_test1, predictions, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "We'll apply k fold cross validation to evaluate the performance of the existing models and be sure of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying on Random Forest Model\n",
    "scores = cross_validation.cross_val_score(estimator = forest, X = X_train, y = Y_train, cv = 10, n_jobs = -1, scoring='mean_squared_error')\n",
    "scores.mean()\n",
    "#scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying on Gradient Boost Model\n",
    "scores = cross_validation.cross_val_score(estimator = gbm, X = X_train, y = Y_train, cv = 10, n_jobs = -1, scoring='mean_squared_error')\n",
    "scores.mean()\n",
    "#scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying on Decision Tree Model\n",
    "scores = cross_validation.cross_val_score(estimator = clf, X = X_train, y = Y_train, cv = 10, n_jobs = -1, scoring='mean_squared_error')\n",
    "scores.mean()\n",
    "#scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying on Random Forest Model\n",
    "scores = cross_validation.cross_val_score(estimator = xbg, X = X_train, y = Y_train, cv = 10, n_jobs = -1, scoring='mean_squared_error')\n",
    "scores.mean()\n",
    "#scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model Performance\n",
    "Hyper parameter tuning with Grid Search.\n",
    "Here we applied for RandomForest, Feel free to apply for other models as well. Grid search helps us know if we need to use linear or non-linear ML model. The main usage is to tell the optimum values of the parameters to be used to improve the performance of the existing model. After we get the best parameters based on criterion here we've taken MSE, we can feed in those along with the nearby values to it again to get the even more better parameters to build the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying Grid Search on RandomForest for different values of estimators,  max features used while splitting\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_estimators' : [500, 1000, 1200, 1500], 'max_features' : [\"auto\", \"sqrt\", \"log2\"]}]\n",
    "grid_search = GridSearchCV(estimator = forest, param_grid = parameters, scoring = 'mean_squared_error', cv = 10)\n",
    "grid_search = grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_score = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
